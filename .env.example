# Ollama Configuration
# Make sure to install and run Ollama first: https://ollama.ai
# Pull the qwen3-vl model: ollama pull qwen3-vl:8b

OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_PROTOCOL=http
OLLAMA_MODEL=qwen3-vl:8b

# Optional: Set your specific model if different
# OLLAMA_MODEL=llama3.1